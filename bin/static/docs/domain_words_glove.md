
[è¿”å›](/mag/doc_detail/main)

---

# é¢†åŸŸè¯æ±‡è¯†åˆ«ä¸è¯å‘é‡è®­ç»ƒ

> æœ¬éƒ¨åˆ†è¯¦ç»†ä»‹ç»ç³»ç»Ÿå†…ç½®çš„**é¢†åŸŸè¯æ±‡è¯†åˆ«æ¨¡å‹**å’Œ**GloVeè¯å‘é‡è‡ªå®šä¹‰è®­ç»ƒ**åŠŸèƒ½ã€‚è¿™äº›åŠŸèƒ½ä¸“ä¸ºé€‚åº”å‚ç›´ä¸“ä¸šé¢†åŸŸçš„ç‹¬ç‰¹ç§æœ‰çŸ¥è¯†è€Œç²¾å¿ƒè®¾è®¡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ä¸“ä¸šæ–‡æœ¬çš„å¤„ç†æ•ˆæœã€‚

---

## ğŸ¯ é¢†åŸŸè¯æ±‡è¯†åˆ«

### åŠŸèƒ½æ¦‚è¿°
ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ä»¥ä¸­æ–‡è¯ç»„ä½œä¸ºTokençš„åŸºæœ¬å•å…ƒï¼Œå› æ­¤ä¸­æ–‡åˆ†è¯çš„è´¨é‡ç›´æ¥å½±å“åç»­æ‰€æœ‰æ–‡æœ¬åˆ†æä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿçš„ä¸­æ–‡åˆ†è¯æ¨¡å‹å¾€å¾€éš¾ä»¥è¯†åˆ«ä¸“ä¸šã€å‚ç›´é¢†åŸŸä¸­å­˜åœ¨çš„å¤§é‡é¢†åŸŸç‰¹å®šè¯æ±‡ã€‚

æœ¬ç³»ç»Ÿçš„**é¢†åŸŸè¯æ±‡è¯†åˆ«**åŠŸèƒ½ä¸“é—¨ç”¨äºï¼š
- ğŸ” è¯†åˆ«å¤§è§„æ¨¡ä¸“ä¸šæ–‡æœ¬åº“ä¸­å¯èƒ½å­˜åœ¨çš„é¢†åŸŸè¯æ±‡
- ğŸ“š è‡ªåŠ¨æ”¶å½•åˆ†è¯æ¨¡å‹æœªè¦†ç›–çš„ä¸“ä¸šæœ¯è¯­
- ğŸš€ æå‡åç»­åˆ†è¯æ¨¡å‹å’Œè¯å‘é‡è®­ç»ƒçš„æ•ˆæœ
- âš¡ æ˜¾è‘—æ”¹å–„BM25æ£€ç´¢ç®—æ³•çš„æ€§èƒ½

### æŠ€æœ¯ç‰¹ç‚¹
- **é«˜æ€§èƒ½æ¨¡å‹**ï¼šå†…ç½®è‡ªç ”çš„é¢†åŸŸè¯æ±‡è¯†åˆ«æ¨¡å‹ï¼Œæ˜¯ç›®å‰å¸‚é¢ä¸Šæœ€å¿«çš„è¯†åˆ«æ¨¡å‹ä¹‹ä¸€
- **æ™ºèƒ½è¯†åˆ«**ï¼šèƒ½å¤Ÿç©·ä¸¾æ‰€æœ‰å¯èƒ½çš„è¯æ±‡ç»„åˆï¼Œç¡®ä¿ä¸é—æ¼é‡è¦é¢†åŸŸæœ¯è¯­
- **å†…å­˜ä¼˜åŒ–**ï¼šé’ˆå¯¹å¤§è§„æ¨¡æ–‡æœ¬åº“è¿›è¡Œäº†å†…å­˜ä½¿ç”¨ä¼˜åŒ–

### âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹
> **å†…å­˜æ¶ˆè€—è¯´æ˜**ï¼šè¯†åˆ«è¿‡ç¨‹éœ€è¦ç©·ä¸¾æ‰€æœ‰å¯èƒ½çš„è¯æ±‡ç»„åˆï¼Œå¯èƒ½æ¶ˆè€—è¾ƒå¤šå†…å­˜ã€‚
> 
> **å‚è€ƒæ•°æ®**ï¼šé’ˆå¯¹1000ç¯‡è®ºæ–‡çš„è¯†åˆ«ä»»åŠ¡çº¦æ¶ˆè€—10GBå†…å­˜ã€‚ä½¿ç”¨å‰è¯·è¯„ä¼°æ•°æ®è§„æ¨¡å’ŒæœåŠ¡å™¨å†…å­˜é…ç½®ã€‚

---

## ğŸ§  è‡ªå®šä¹‰è¯å‘é‡åº“è®­ç»ƒ

### åŠŸèƒ½è¯´æ˜
**è‡ªå®šä¹‰è¯å‘é‡åº“è®­ç»ƒ**æ˜¯æŒ‡åŸºäºæ–‡æœ¬åº“è‡ªå®šä¹‰è®­ç»ƒGloVeæ–‡æœ¬å‘é‡åŒ–æ¨¡å‹ã€‚å½“å®Œæˆé¢†åŸŸè¯æ±‡è¯†åˆ«åï¼Œç³»ç»Ÿå†…ç½®çš„é¢„è®­ç»ƒè¯å‘é‡åº“å¯èƒ½æ— æ³•å‘é‡åŒ–æ–°è¯†åˆ«çš„é¢†åŸŸè¯æ±‡ï¼Œæ­¤æ—¶å¯ä»¥ï¼š

- ğŸ¯ è®­ç»ƒé«˜åº¦ä½“ç°ç”¨æˆ·é¢†åŸŸæ•°æ®ç‰¹ç‚¹çš„è‡ªå®šä¹‰GloVeæ¨¡å‹
- ğŸ“ˆ åœ¨é¢†åŸŸæ–‡æœ¬ææ–™ä¸Šè·å¾—æ›´å¥½çš„æ£€ç´¢æ•ˆæœ
- ğŸ”„ è‡ªåŠ¨æ›¿æ¢æ–‡æœ¬åº“çš„å…¨å±€é¢„è®­ç»ƒæ¨¡å‹

### æ¨¡å‹æ›¿æ¢æœºåˆ¶
| æ–‡æœ¬åº“è®¾ç½® | æ¨¡å‹ä½¿ç”¨æƒ…å†µ |
|------------|--------------|
| GloVeå‘é‡åŒ–æ¨¡å‹ | âœ… è‡ªåŠ¨ä½¿ç”¨è‡ªå®šä¹‰GloVeæ¨¡å‹ |
| DistillBertå‘é‡åŒ–æ¨¡å‹ | âš ï¸ ä¸å—å½±å“ï¼Œä»ä½¿ç”¨åŸæ¨¡å‹ |

### æŠ€æœ¯é™åˆ¶è¯´æ˜
> **å½“å‰é™åˆ¶**ï¼šæœåŠ¡ç«¯ä¾§æš‚æ—¶æ— æ³•è‡ªè®­ç»ƒDistillBertæ¨¡å‹
> 
> **åŸå› åˆ†æ**ï¼š
> - BERTæ¨¡å‹è§„æ¨¡è¿‡å¤§
> - éœ€è¦å¤§é‡GPUè®¾å¤‡é›†ç¾¤è®­ç»ƒ
> - æœåŠ¡ç«¯è®¡ç®—èµ„æºæ— æ³•èƒœä»»

---

## ğŸ“‹ ä½¿ç”¨æµç¨‹

```mermaid
graph TD
    A[å¯¼å…¥ä¸“ä¸šæ–‡æœ¬åº“] --> B[æ‰§è¡Œé¢†åŸŸè¯æ±‡è¯†åˆ«]
    B --> C[è¯†åˆ«é¢†åŸŸç‰¹å®šè¯æ±‡]
    C --> D[è®­ç»ƒè‡ªå®šä¹‰GloVeæ¨¡å‹]
    D --> E[è‡ªåŠ¨æ›¿æ¢é¢„è®­ç»ƒæ¨¡å‹]
    E --> F[æå‡æ£€ç´¢æ•ˆæœ]
```

---

## ğŸ”§ æœ€ä½³å®è·µå»ºè®®

1. **æ•°æ®è§„æ¨¡è¯„ä¼°**ï¼šä½¿ç”¨å‰è¯„ä¼°æ–‡æœ¬åº“å¤§å°å’ŒæœåŠ¡å™¨å†…å­˜é…ç½®
2. **åˆ†æ‰¹å¤„ç†**ï¼šå¯¹äºè¶…å¤§è§„æ¨¡æ–‡æœ¬åº“ï¼Œå»ºè®®åˆ†æ‰¹è¿›è¡Œè¯†åˆ«å’Œè®­ç»ƒ
3. **æ•ˆæœéªŒè¯**ï¼šå®Œæˆè®­ç»ƒåï¼Œå¯¹æ¯”æµ‹è¯•æ£€ç´¢æ•ˆæœæå‡æƒ…å†µ
4. **å®šæœŸæ›´æ–°**ï¼šéšç€é¢†åŸŸçŸ¥è¯†æ›´æ–°ï¼Œå®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹

---

---

# Domain Vocabulary Recognition and Word Vector Training

> This section provides a detailed introduction to the system's built-in **Domain Vocabulary Recognition Model** and **Custom GloVe Word Vector Training** functionality. These features are specially designed to adapt to unique private knowledge in vertical professional domains, significantly improving the processing effectiveness of professional texts.

---

## ğŸ¯ Domain Vocabulary Recognition

### Function Overview
Chinese natural language processing uses Chinese word groups as the basic units of Tokens, so the quality of Chinese word segmentation directly affects the accuracy of all subsequent text analysis tasks. Traditional Chinese word segmentation models often struggle to identify large numbers of domain-specific vocabulary that exist in professional, vertical domains.

The **Domain Vocabulary Recognition** function of this system is specifically designed to:
- ğŸ” Identify domain vocabulary that may exist in large-scale professional text libraries
- ğŸ“š Automatically include professional terms not covered by segmentation models
- ğŸš€ Improve the effectiveness of subsequent segmentation models and word vector training
- âš¡ Significantly enhance the performance of BM25 retrieval algorithms

### Technical Features
- **High-Performance Model**: Built-in self-developed domain vocabulary recognition model, one of the fastest recognition models currently available
- **Intelligent Recognition**: Capable of exhaustively enumerating all possible vocabulary combinations to ensure no important domain terms are missed
- **Memory Optimization**: Memory usage optimized for large-scale text libraries

### âš ï¸ Usage Considerations
> **Memory Consumption Note**: The recognition process requires exhaustive enumeration of all possible vocabulary combinations, which may consume significant memory.
> 
> **Reference Data**: Recognition tasks for 1000 papers consume approximately 10GB of memory. Please evaluate data scale and server memory configuration before use.

---

## ğŸ§  Custom Word Vector Library Training

### Function Description
**Custom Word Vector Library Training** refers to custom training of GloVe text vectorization models based on text libraries. After completing domain vocabulary recognition, the system's built-in pre-trained word vector library may not be able to vectorize newly identified domain vocabulary. At this point, you can:

- ğŸ¯ Train custom GloVe models that highly reflect the characteristics of user domain data
- ğŸ“ˆ Achieve better retrieval effects on domain text materials
- ğŸ”„ Automatically replace the text library's global pre-trained model

### Model Replacement Mechanism
| Text Library Setting | Model Usage |
|---------------------|-------------|
| GloVe Vectorization Model | âœ… Automatically uses custom GloVe model |
| DistillBert Vectorization Model | âš ï¸ Unaffected, still uses original model |

### Technical Limitations
> **Current Limitations**: Service-side custom training of DistillBert models is temporarily unavailable
> 
> **Reasons**:
> - BERT model size is too large
> - Requires large-scale GPU device cluster training
> - Service-side computing resources cannot handle the task

---

## ğŸ“‹ Usage Workflow

```mermaid
graph TD
    A[Import Professional Text Library] --> B[Execute Domain Vocabulary Recognition]
    B --> C[Identify Domain-Specific Vocabulary]
    C --> D[Train Custom GloVe Model]
    D --> E[Automatically Replace Pre-trained Model]
    E --> F[Improve Retrieval Effectiveness]
```

---

## ğŸ”§ Best Practice Recommendations

1. **Data Scale Assessment**: Evaluate text library size and server memory configuration before use
2. **Batch Processing**: For ultra-large-scale text libraries, consider batch recognition and training
3. **Effectiveness Verification**: After completing training, compare and test retrieval effectiveness improvements
4. **Regular Updates**: Re-train models periodically as domain knowledge updates

---

## ğŸ“š Related Documentation

- [æ–‡æœ¬åº“ç®¡ç†](/mag/doc_detail/text_library)
- [æ£€ç´¢ç®—æ³•é…ç½®](/mag/doc_detail/retrieval_config)
- [æ¨¡å‹æ€§èƒ½ä¼˜åŒ–](/mag/doc_detail/model_optimization)